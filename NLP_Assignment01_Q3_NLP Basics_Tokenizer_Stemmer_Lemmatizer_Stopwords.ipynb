{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\YOGITA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\YOGITA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tokenizer_func.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tokenizer_func.py\n",
    "\n",
    "#import libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from operator import itemgetter\n",
    "\n",
    "#define tokenize function\n",
    "def Tokenize():\n",
    "    text = input()\n",
    "    tokens = word_tokenize(text) #tokenize the words\n",
    "    fdist = FreqDist(tokens) #find the frequency of words\n",
    "    for f in fdist:\n",
    "        print(f, fdist[f])\n",
    "    min_5 = dict(sorted(fdist.items(), key=itemgetter(1))[:5]) #find the least 5 occuring words\n",
    "    print(\"Least 5 occurring tokens:\", min_5)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9ed1fc93ce42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#run the Tokenize function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtokenizer_func\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mTokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Python\\desktop\\YS\\NITW\\Module-07 NLP\\Assignment 01\\tokenizer_func.py\u001b[0m in \u001b[0;36mTokenize\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#define tokenize function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mTokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#tokenize the words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mfdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#find the frequency of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw_input' is not defined"
     ]
    }
   ],
   "source": [
    "#run the Tokenize function\n",
    "from tokenizer_func import Tokenize\n",
    "Tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting removestopwords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile removestopwords.py\n",
    "#Remove stop words function\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "def RemoveStopWords(token_list):\n",
    "    stop_words = set(stopwords.words('english')) #copy stopwords to a list\n",
    "    stop_removed = [w for w in token_list if w not in stop_words] #for loop to append the words to a list except stopwords\n",
    "    return stop_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\YOGITA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "['is', 'a', 'of', 'and', 'with', 'the', 'between', 'and', 'in', 'how', 'to', 'to', 'and', 'of'] \n",
      "==========================================\n",
      "Frequency Distribution of Stop Words:\n",
      "==========================================\n",
      "and: 3\n",
      "of: 2\n",
      "to: 2\n",
      "is: 1\n",
      "a: 1\n",
      "with: 1\n",
      "the: 1\n",
      "between: 1\n",
      "in: 1\n",
      "how: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\YOGITA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#counting frequency of stop words in a string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "string = input()\n",
    "from removestopwords import RemoveStopWords\n",
    "\n",
    "tokens = word_tokenize(string) #tokenize\n",
    "stop_removed = RemoveStopWords(tokens) #remove stopwords\n",
    "\n",
    "stop_present = [word for word in tokens if word not in stop_removed] #save stop words to a list\n",
    "print(stop_present,'\\n==========================================')\n",
    "\n",
    "swfdist = FreqDist(stop_present) #find the frequency of stop words\n",
    "print(\"Frequency Distribution of Stop Words:\\n==========================================\")\n",
    "for f in swfdist:\n",
    "    print(\"{}: {}\".format(f, swfdist[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Frequency of Stop words in the input')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot a bar graph depicting frequency of stop words\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(swfdist.keys(), swfdist.values())\n",
    "plt.title(\"Frequency of Stop words in the input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lemma_stem.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lemma_stem.py\n",
    "#lemmatize and stemming\n",
    "#import libraries\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "#define lemmatize function\n",
    "def Lemmatize(token_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in token_list]\n",
    "    return lemmatized\n",
    "\n",
    "#define stemming function\n",
    "def Stemmed(token_list):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(word) for word in token_list]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\YOGITA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Original Word Lemmatized Form Stemmed Form\n",
      "0          cries             cry          cri\n",
      "1        studies           study        studi\n",
      "2        studied         studied        studi\n",
      "3          cried           cried          cri\n",
      "4          hurry           hurry        hurri\n",
      "5        hurried         hurried        hurri\n",
      "6        hurries           hurry        hurri\n",
      "7           jump            jump         jump\n",
      "8         jumped          jumped         jump\n",
      "9        jumping         jumping         jump\n",
      "10         jumps            jump         jump\n"
     ]
    }
   ],
   "source": [
    "#implement lemmatizer and stemmer\n",
    "text = \"cries studies studied cried hurry hurried hurries jump jumped jumping jumps\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "import lemma_stem\n",
    "from lemma_stem import Lemmatize, Stemmed\n",
    "lemma = Lemmatize(tokens)\n",
    "stem = Stemmed(tokens)\n",
    "\n",
    "#write the results to a Pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['Original Word'] = tokens\n",
    "df['Lemmatized Form'] = lemma\n",
    "df['Stemmed Form'] = stem\n",
    "\n",
    "print(df)\n",
    "df.to_csv(\"StemmedAndLemmatizedResults.csv\") #save the results to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PreProcess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PreProcess.py\n",
    "#creating a PreProcess file\n",
    "from tokenizer_func import Tokenize\n",
    "from removestopwords import RemoveStopWords\n",
    "from lemma_stem import Lemmatize\n",
    "\n",
    "#defining Refine function to carry out the processing\n",
    "def Refine():\n",
    "    tokens = Tokenize()\n",
    "    stop_remove = RemoveStopWords(tokens)\n",
    "    lemmatized_words = Lemmatize(stop_remove)\n",
    "    print('Tokenized: ', tokens)\n",
    "    print('Stop Words Removed Copy: ', stop_remove)\n",
    "    print('Lemmatized Words: ', lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "language 3\n",
      ", 3\n",
      "and 3\n",
      "of 2\n",
      "computers 2\n",
      "to 2\n",
      "Natural 1\n",
      "processing 1\n",
      "( 1\n",
      "NLP 1\n",
      ") 1\n",
      "is 1\n",
      "a 1\n",
      "subfield 1\n",
      "linguistics 1\n",
      "computer 1\n",
      "science 1\n",
      "artificial 1\n",
      "intelligence 1\n",
      "concerned 1\n",
      "with 1\n",
      "the 1\n",
      "interactions 1\n",
      "between 1\n",
      "human 1\n",
      "in 1\n",
      "particular 1\n",
      "how 1\n",
      "program 1\n",
      "process 1\n",
      "analyze 1\n",
      "large 1\n",
      "amounts 1\n",
      "natural 1\n",
      "data 1\n",
      ". 1\n",
      "Least 5 occurring tokens: {'Natural': 1, 'processing': 1, '(': 1, 'NLP': 1, ')': 1}\n",
      "Tokenized:  ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n",
      "Stop Words Removed Copy:  ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.']\n",
      "Lemmatized Words:  ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', ',', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "#runt the refine function\n",
    "from PreProcess import Refine\n",
    "Refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
